<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
<title>Blink Squares — MediaPipe FaceMesh (blink.html)</title>
<style>
  :root{--bg:#fbfbfb;--muted:#666}
  html,body{height:100%;margin:0;background:var(--bg);font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;color:#111}
  .wrap{min-height:100%;display:flex;flex-direction:column;align-items:center;justify-content:center;gap:12px;padding:20px;box-sizing:border-box}
  h1{margin:0;font-size:20px;font-weight:600}
  .squares{display:flex;gap:12px;align-items:center;justify-content:center}
  .sq{width:92px;height:92px;border-radius:12px;background:#111;display:flex;align-items:center;justify-content:center;color:#fff;font-weight:700;box-shadow:0 8px 20px rgba(12,12,12,.12);font-size:14px}
  .controls{display:flex;gap:12px;align-items:center;flex-wrap:wrap;justify-content:center}
  #startBtn{padding:12px 18px;font-size:16px;border-radius:10px;border:0;background:#111;color:#fff;cursor:pointer}
  #status{font-size:13px;color:var(--muted);max-width:720px;text-align:center}
  #errorBox{font-size:13px;color:#b00020;max-width:720px;text-align:left;white-space:pre-wrap}
  #videoEl{display:none}
  canvas{display:none}
  .note{font-size:13px;color:var(--muted);max-width:640px;text-align:center}
  @media (max-width:420px){ .sq{width:74px;height:74px;font-size:12px} #startBtn{font-size:15px} }
  pre#debug{font-family:monospace;font-size:12px;color:#222;max-width:720px;white-space:pre-wrap;text-align:left}
</style>
</head>
<body>
<div class="wrap">
  <h1>Blink Squares — Left / Right / Both (MediaPipe)</h1>

  <div class="squares" aria-hidden="true">
    <div id="leftSq" class="sq" aria-label="left">LEFT</div>
    <div id="bothSq" class="sq" aria-label="both">BOTH</div>
    <div id="rightSq" class="sq" aria-label="right">RIGHT</div>
  </div>

  <div class="controls">
    <button id="startBtn">Start camera</button>
    <div id="status">Click Start. Required files (must be hosted locally): <code>libs/mediapipe/face_mesh.js</code> and the wasm files in the same folder.</div>
  </div>

  <div id="errorBox" aria-live="polite"></div>
  <pre id="debug"></pre>

  <p class="note">Place these files in your repo: <code>libs/mediapipe/face_mesh.js</code>, <code>libs/mediapipe/face_mesh_solution_wasm_bin.wasm</code>, <code>libs/mediapipe/face_mesh_solution_simd_wasm_bin.wasm</code>. Then open this page via GitHub Pages (HTTPS).</p>

  <video id="videoEl" autoplay playsinline muted></video>
  <canvas id="hiddenCanvas"></canvas>
</div>

<script>
/* blink.html — MediaPipe FaceMesh local-only version
   REQUIRED files (exact paths relative to page):
     libs/mediapipe/face_mesh.js
     libs/mediapipe/face_mesh_solution_wasm_bin.wasm
     libs/mediapipe/face_mesh_solution_simd_wasm_bin.wasm

   Behavior:
   - Start on user click -> request camera -> initialize FaceMesh with locateFile mapping -> run detection loop
   - Detect left/right/both eye closures (EAR method) and show persistent colored square until next event
   - Works on desktop/mobile Safari & Chrome assuming files are present and page served over HTTPS
*/

const startBtn = document.getElementById('startBtn');
const statusEl = document.getElementById('status');
const errorBox = document.getElementById('errorBox');
const debugEl = document.getElementById('debug');
const leftSq = document.getElementById('leftSq');
const rightSq = document.getElementById('rightSq');
const bothSq = document.getElementById('bothSq');
const videoEl = document.getElementById('videoEl');
const canvas = document.getElementById('hiddenCanvas');

function log(...args){ debugEl.textContent += args.map(a => (typeof a === 'object' ? JSON.stringify(a) : String(a))).join(' ') + '\n'; console.log(...args); }
function setStatus(s){ statusEl.textContent = s; log('[STATUS]', s); }
function setError(s){ errorBox.textContent = s; log('[ERROR]', s); }

function showEvent(ev){
  const COLORS = { none:'#111', left:'#ff6b6b', right:'#63c2ff', both:'#ffd36b' };
  leftSq.style.background = ev === 'left' ? COLORS.left : COLORS.none;
  rightSq.style.background = ev === 'right' ? COLORS.right : COLORS.none;
  bothSq.style.background = ev === 'both' ? COLORS.both : COLORS.none;
}
showEvent('none');

// Eye indices for MediaPipe FaceMesh (canonical)
const LEFT_EYE_INDICES  = [33,160,158,133,153,144];
const RIGHT_EYE_INDICES = [362,385,387,263,373,380];

function toPoint(p){ if(Array.isArray(p)) return {x:p[0], y:p[1]}; return {x:p.x, y:p.y}; }
function dist(a,b){ const dx=a.x-b.x, dy=a.y-b.y; return Math.hypot(dx,dy); }
function computeEAR(landmarks, idxs){
  const p1 = toPoint(landmarks[idxs[0]]);
  const p2 = toPoint(landmarks[idxs[1]]);
  const p3 = toPoint(landmarks[idxs[2]]);
  const p4 = toPoint(landmarks[idxs[3]]);
  const p5 = toPoint(landmarks[idxs[4]]);
  const p6 = toPoint(landmarks[idxs[5]]);
  const A = dist(p2,p6);
  const B = dist(p3,p5);
  const C = dist(p1,p4) || 1e-6;
  return (A + B) / (2 * C);
}

// detection state
let running = false;
let closingLeft = false, closingRight = false;
let lastEvent = 'none';
let EAR_THRESHOLD = 0.18; // starting value (tweak via console: window._blink.EAR=0.16)

let faceMesh = null;

// locateFile mapping: MediaPipe will request various asset filenames; map them to files you host.
// We map anything containing "wasm" to the wasm binaries we asked you to host (two variants).
function locateFileMapper(file){
  // if file name contains "simd" prefer simd binary
  if(typeof file === 'string' && file.toLowerCase().includes('simd')) {
    return '/libs/mediapipe/face_mesh_solution_simd_wasm_bin.wasm';
  }
  if(typeof file === 'string' && file.toLowerCase().includes('wasm')) {
    return '/libs/mediapipe/face_mesh_solution_wasm_bin.wasm';
  }
  // default: map to the same basename in libs/mediapipe/
  return '/libs/mediapipe/' + file;
}

async function requestCamera(){
  setError(''); setStatus('Requesting camera (tap allow)…');
  try{
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: 'user', width: { ideal: 640 }, height: { ideal: 480 } },
      audio: false
    });
    videoEl.srcObject = stream;
    await new Promise(r => videoEl.onloadedmetadata = r);
    setStatus('Camera started');
    return stream;
  } catch(err){
    setError('Camera error: ' + (err && err.name ? err.name + ' — ' + err.message : String(err)));
    throw err;
  }
}

// main initialization: assumes libs/mediapipe/face_mesh.js is reachable and defines FaceMesh global
async function initFaceMeshAndRun(){
  if(typeof FaceMesh !== 'function'){
    setError('face_mesh.js not found or did not define FaceMesh. Ensure libs/mediapipe/face_mesh.js is present and accessible.');
    return false;
  }

  try{
    setStatus('Initializing MediaPipe FaceMesh — loading runtime assets from ./libs/mediapipe/ ...');
    faceMesh = new FaceMesh({
      locateFile: (file) => locateFileMapper(file)
    });

    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });

    faceMesh.onResults(onFaceMeshResults);

    // start send loop
    running = true;
    detectLoopWithFaceMesh();
    return true;
  } catch(err){
    setError('FaceMesh initialization error: ' + (err && err.message ? err.message : String(err)));
    console.error(err);
    return false;
  }
}

async function detectLoopWithFaceMesh(){
  setStatus('Detecting — blink/wink will light the squares');
  try{
    while(running){
      if(videoEl.readyState >= 2){
        try{
          await faceMesh.send({image: videoEl});
        } catch(e){
          console.error('faceMesh.send error', e);
          setError('FaceMesh runtime error: ' + (e && e.message ? e.message : String(e)) + '\nCheck that wasm files are present in libs/mediapipe/ and accessible.');
          running = false;
          break;
        }
      }
      await new Promise(r => requestAnimationFrame(r));
    }
  } catch(err){
    console.error('FaceMesh loop error', err);
    setError('FaceMesh loop error: ' + (err && err.message ? err.message : String(err)));
    running = false;
  }
}

function onFaceMeshResults(results){
  if(!results || !results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0){
    setStatus('No face detected — please face the camera');
    return;
  }
  const lm = results.multiFaceLandmarks[0]; // array of {x,y,z}
  processLandmarks(lm);
}

function processLandmarks(landmarks){
  try{
    const leftEAR = computeEAR(landmarks, LEFT_EYE_INDICES);
    const rightEAR = computeEAR(landmarks, RIGHT_EYE_INDICES);
    const leftClosed = leftEAR < EAR_THRESHOLD;
    const rightClosed = rightEAR < EAR_THRESHOLD;

    if(leftClosed) closingLeft = true;
    if(rightClosed) closingRight = true;

    if(!leftClosed && !rightClosed && (closingLeft || closingRight)){
      if(closingLeft && closingRight) lastEvent = 'both';
      else if(closingLeft) lastEvent = 'left';
      else if(closingRight) lastEvent = 'right';
      else lastEvent = 'both';
      showEvent(lastEvent);
      setStatus('Detected: ' + lastEvent.toUpperCase() + `  (L:${leftEAR.toFixed(3)} R:${rightEAR.toFixed(3)})`);
      closingLeft = closingRight = false;
    } else {
      // update small debug EAR values
      setStatus(`EAR L:${leftEAR.toFixed(3)} R:${rightEAR.toFixed(3)}`);
    }
  } catch(err){
    console.error('processLandmarks error', err);
  }
}

// start flow handler
startBtn.addEventListener('click', async () => {
  startBtn.disabled = true;
  setError(''); debugEl.textContent = '';
  setStatus('Start pressed — requesting camera (user gesture)...');

  try{
    await requestCamera();
  } catch(e){
    startBtn.disabled = false;
    return;
  }

  const ok = await initFaceMeshAndRun();
  if(!ok) startBtn.disabled = false;
});

// small console helpers
window._blink = {
  stop: () => { running = false; setStatus('Stopped'); },
  EAR: (v) => { EAR_THRESHOLD = Number(v); log('EAR set to', EAR_THRESHOLD); }
};
</script>
</body>
</html>
